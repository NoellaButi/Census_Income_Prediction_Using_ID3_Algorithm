{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc820162-e571-4fa8-9de3-193a8bd0d857",
   "metadata": {},
   "source": [
    "# Census Income Prediction Using ID3 Algorithm\n",
    "\n",
    "## Dataset Description: Census Income Prediction\n",
    "\n",
    "This dataset predicts whether an individual's yearly income exceeds $50,000 based on various demographic attributes from the 1994 U.S. Census Bureau database. It is used for supervised learning to classify income levels into two categories: \"≤50K\" and \">50K.\"\n",
    "\n",
    "### Features and Class Label\n",
    "\n",
    "- **age**: The age of the individual (categorical: transformed from a continuous variable).\n",
    "- **workclass**: The type of employment (e.g., Private, Self-Employed, Government, etc.) (categorical).\n",
    "- **education**: The highest level of education attained (e.g., Bachelors, Masters, etc.) (categorical).\n",
    "- **education_num**: The number of years of education (categorical: transformed from a continuous variable).\n",
    "- **marital_status**: Marital status of the individual (e.g., Married, Single, Divorced) (categorical).\n",
    "- **occupation**: The individual's occupation (e.g., Tech, Healthcare, etc.) (categorical).\n",
    "- **relationship**: Relationship status (e.g., Husband, Wife, Not-in-family) (categorical).\n",
    "- **race**: Racial classification (e.g., White, Black, Asian) (categorical).\n",
    "- **sex**: Gender of the individual (categorical).\n",
    "- **capital_gain**: The capital gain from investments (removed from the dataset).\n",
    "- **capital_loss**: The capital loss from investments (removed from the dataset).\n",
    "- **hours_per_week**: The number of hours worked per week (categorical: transformed from a continuous variable).\n",
    "- **native_country**: Country of origin (e.g., United States, Canada, etc.) (categorical).\n",
    "- **high_income**: Indicates whether the individual's income is ≤50K or >50K (class label).\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal of this assignment is to build a decision tree using the ID3 algorithm to classify whether a person’s yearly income is ≤50K or >50K. The tree should be constructed based on entropy and information gain calculations.\n",
    "\n",
    "### Implementation Steps\n",
    "1. **Data Preprocessing**:\n",
    "   - Load the training and test datasets: `census_training.csv` and `census_training_test.csv`.\n",
    "   - Verify that the datasets have been cleaned and structured as described, with no missing or unknown data.\n",
    "\n",
    "2. **Helper Functions**:\n",
    "   - Create functions to calculate:\n",
    "     - **Entropy**: To measure the uncertainty in the dataset.\n",
    "     - **Information Gain**: To determine which attribute to split on for building the tree.\n",
    "     - **Best Feature Selection**: To find the attribute with the highest information gain for splitting.\n",
    "\n",
    "3. **Building the Decision Tree**:\n",
    "   - Implement the ID3 algorithm from scratch, utilizing the helper functions to recursively build the tree.\n",
    "   - Save the tree as a dictionary of dictionaries for easier manipulation and visualization.\n",
    "\n",
    "4. **Validation**:\n",
    "   - Test the algorithm on the smaller datasets `playtennis.csv` and `emails.csv` to ensure the tree structure matches expected results.\n",
    "   - Verify that your implementation correctly builds the decision trees for these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "439b2f3b-89ae-4ec6-a125-4d563aeef633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate entropy of a target column\n",
    "def calculate_entropy(data, target_column):\n",
    "    # Count unique values in the target column and their frequencies\n",
    "    counts = data[target_column].value_counts()\n",
    "    # Calculate probabilities\n",
    "    probabilities = counts / counts.sum()\n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcdb03c-83e9-456b-8c71-d7f383140b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(data, feature, target_column):\n",
    "    # Calculate the total entropy of the target column\n",
    "    total_entropy = calculate_entropy(data, target_column)\n",
    "    \n",
    "    # Get unique values and their counts for the feature\n",
    "    counts = data[feature].value_counts(normalize=True)\n",
    "    \n",
    "    # Calculate the weighted entropy\n",
    "    weighted_entropy = sum(count * calculate_entropy(data[data[feature] == value], target_column)\n",
    "                           for value, count in counts.items())\n",
    "    \n",
    "    # Calculate information gain\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcdfd4c-704a-4d20-8cc3-a508d7aa6e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlayTennis Data:\n",
      "    Outlook Temperature Humidity    Wind PlayTennis\n",
      "0     Sunny         Hot     High    Weak         No\n",
      "1     Sunny         Hot     High  Strong         No\n",
      "2  Overcast         Hot     High    Weak        Yes\n",
      "3      Rain        Mild     High    Weak        Yes\n",
      "4      Rain        Cool   Normal    Weak        Yes\n",
      "\n",
      "Emails Data:\n",
      "   SUSPICIOUS WORDS  UNKNOWN SENDER  CONTAINS IMAGES CLASS\n",
      "0              True           False             True  spam\n",
      "1              True            True            False  spam\n",
      "2              True            True            False  spam\n",
      "3             False            True             True   ham\n",
      "4             False           False            False   ham\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the PlayTennis dataset and exclude the 'Day' column\n",
    "playtennis_data = pd.read_csv('data/playtennis.csv').drop(columns='Day')\n",
    "\n",
    "# Load the Emails dataset and exclude the 'ID' column\n",
    "emails_data = pd.read_csv('data/emails.csv').drop(columns='ID')\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "print(\"PlayTennis Data:\")\n",
    "print(playtennis_data.head())\n",
    "print(\"\\nEmails Data:\")\n",
    "print(emails_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5012699-86b2-427b-a6a9-57d97084b575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for Outlook: 0.247\n",
      "Information Gain for Temperature: 0.029\n",
      "Information Gain for Humidity: 0.152\n",
      "Information Gain for Wind: 0.048\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "features = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
    "target = 'PlayTennis'\n",
    "\n",
    "# Calculate and display information gain for each feature\n",
    "for feature in features:\n",
    "    ig = calculate_information_gain(playtennis_data, feature, target)\n",
    "    print(f\"Information Gain for {feature}: {ig:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73077fd0-e34a-4a89-aa7d-5fcd690c5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        # Initialize the tree with a maximum depth\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, data, target_column):\n",
    "        # Build the decision tree from the training data\n",
    "        self.tree = self._build_tree(data, target_column)\n",
    "\n",
    "    def _build_tree(self, data, target_column, depth=0):\n",
    "        # If all target values are the same, return that value\n",
    "        if len(np.unique(data[target_column])) == 1:\n",
    "            return data[target_column].iloc[0]\n",
    "\n",
    "        # If max depth is reached, return the most common target value\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return data[target_column].mode()[0]\n",
    "\n",
    "        # Identify the best feature to split the data\n",
    "        best_feature = self._best_feature(data, target_column)\n",
    "        tree = {best_feature: {}}\n",
    "\n",
    "        # Split data and build subtrees for each unique value of the best feature\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            subset = data[data[best_feature] == value]\n",
    "            subtree = self._build_tree(subset, target_column, depth + 1)\n",
    "            tree[best_feature][value] = subtree\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def _best_feature(self, data, target_column):\n",
    "        # Calculate information gain for each feature and return the best one\n",
    "        features = data.columns[data.columns != target_column]  # Exclude target column\n",
    "        gains = {feature: self._calculate_information_gain(data, feature, target_column) for feature in features}\n",
    "        return max(gains, key=gains.get)\n",
    "\n",
    "    def _calculate_entropy(self, data, target_column):\n",
    "        # Calculate the entropy of the target variable\n",
    "        values, counts = np.unique(data[target_column], return_counts=True)\n",
    "        entropy = 0\n",
    "        for count in counts:\n",
    "            probability = count / np.sum(counts)\n",
    "            entropy -= probability * np.log2(probability)\n",
    "        return entropy\n",
    "\n",
    "    def _calculate_information_gain(self, data, feature, target_column):\n",
    "        # Calculate the information gain for a given feature\n",
    "        total_entropy = self._calculate_entropy(data, target_column)\n",
    "        values, counts = np.unique(data[feature], return_counts=True)\n",
    "        weighted_entropy = 0\n",
    "        for i, value in enumerate(values):\n",
    "            subset = data[data[feature] == value]\n",
    "            subset_entropy = self._calculate_entropy(subset, target_column)\n",
    "            weighted_entropy += (counts[i] / np.sum(counts)) * subset_entropy\n",
    "        return total_entropy - weighted_entropy\n",
    "\n",
    "    def get_tree(self):\n",
    "        # Return the decision tree\n",
    "        return self.tree\n",
    "\n",
    "    def print_tree(self, tree=None, depth=0):\n",
    "        # Print the decision tree (for debugging)\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "        print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147e8c72-bf67-4b1d-8d70-f5f993432813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlayTennis Spam Tree (as dictionary):\n",
      "{'Outlook': {'Overcast': 'Yes', 'Rain': {'Wind': {'Strong': 'No', 'Weak': 'Yes'}}, 'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}}}}\n",
      "\n",
      "Emails Spam Tree (as dictionary):\n",
      "{'SUSPICIOUS WORDS': {False: 'ham', True: 'spam'}}\n"
     ]
    }
   ],
   "source": [
    "# Fit and get the decision tree for the PlayTennis dataset\n",
    "# Define relevant features and target variable\n",
    "playtennis_features = playtennis_data[['Outlook', 'Temperature', 'Humidity', 'Wind']]\n",
    "playtennis_target = playtennis_data['PlayTennis']\n",
    "\n",
    "# Create a DecisionTree instance with a maximum depth of 3\n",
    "playtennis_tree = DecisionTree(max_depth=3)\n",
    "\n",
    "# Fit the decision tree model to the PlayTennis data\n",
    "playtennis_tree.fit(playtennis_data.assign(PlayTennis=playtennis_target), target_column='PlayTennis')\n",
    "\n",
    "# Print the structure of the PlayTennis decision tree as a dictionary\n",
    "print(\"PlayTennis Spam Tree (as dictionary):\")\n",
    "print(playtennis_tree.get_tree())\n",
    "\n",
    "# Fit and get the decision tree for the Emails dataset\n",
    "# Define relevant features and target variable\n",
    "emails_features = emails_data[['SUSPICIOUS WORDS', 'UNKNOWN SENDER', 'CONTAINS IMAGES']]\n",
    "emails_target = emails_data['CLASS']\n",
    "\n",
    "# Create a DecisionTree instance with a maximum depth of 3\n",
    "emails_tree = DecisionTree(max_depth=3)\n",
    "\n",
    "# Fit the decision tree model to the Emails data\n",
    "emails_tree.fit(emails_data.assign(CLASS=emails_target), target_column='CLASS')\n",
    "\n",
    "# Print the structure of the Emails decision tree as a dictionary\n",
    "print(\"\\nEmails Spam Tree (as dictionary):\")\n",
    "print(emails_tree.get_tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcfb3d56-3dd9-4235-9775-79573b983b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded PlayTennis decision tree as 'playtennis_decision_tree.pdf'.\n",
      "Successfully downloaded Spam Email decision tree as 'spam_email_decision_tree.pdf'.\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Function to draw the decision tree from a dictionary\n",
    "def draw_decision_tree_dictionary(tree_dictionary):\n",
    "    # Validate input type\n",
    "    if not isinstance(tree_dictionary, dict):\n",
    "        raise TypeError(\"Argument tree_dictionary must be of type dictionary\")\n",
    "    if not tree_dictionary:\n",
    "        raise ValueError(\"Dictionary tree_dictionary is empty\")\n",
    "\n",
    "    # Initialize a new directed graph\n",
    "    dot = Digraph(strict=True)\n",
    "    draw_tree(dot, tree_dictionary, None)  # Call recursive drawing function\n",
    "    return dot\n",
    "\n",
    "def draw_tree(dot, tree_dictionary, parent_node_name):\n",
    "    # Recursively draw the tree from the dictionary\n",
    "    if isinstance(tree_dictionary, dict):\n",
    "        for key in tree_dictionary:\n",
    "            # Clean key by removing spaces\n",
    "            no_spaces_key = str(key).replace(\" \", \"\")\n",
    "            dot.node(no_spaces_key, str(key), shape=\"ellipse\")  # Create a node\n",
    "\n",
    "            # Create edge from parent to current node\n",
    "            if parent_node_name is not None:\n",
    "                dot.edge(parent_node_name, no_spaces_key)\n",
    "\n",
    "            # Recurse to draw child nodes\n",
    "            draw_tree(dot, tree_dictionary[key], no_spaces_key)\n",
    "    else:\n",
    "        # If a leaf node, create a plain node\n",
    "        val = str(tree_dictionary)\n",
    "        dot.node(val, val, shape=\"plain\")\n",
    "        dot.edge(parent_node_name, val)  # Create edge to leaf node\n",
    "\n",
    "# Example decision tree dictionaries\n",
    "playtennis_tree_dict = {\n",
    "    'Outlook': {\n",
    "        'Overcast': 'Yes',\n",
    "        'Rain': {\n",
    "            'Wind': {\n",
    "                'Strong': 'No',\n",
    "                'Weak': 'Yes'\n",
    "            }\n",
    "        },\n",
    "        'Sunny': {\n",
    "            'Humidity': {\n",
    "                'High': 'No',\n",
    "                'Normal': 'Yes'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "spam_email_tree_dict = {\n",
    "    'SUSPICIOUS WORDS': {\n",
    "        False: 'ham',\n",
    "        True: 'spam'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Draw the PlayTennis decision tree and save as PDF\n",
    "playtennis_dot = draw_decision_tree_dictionary(playtennis_tree_dict)\n",
    "playtennis_dot.render(\"playtennis_spam_tree\", format='pdf', cleanup=True)\n",
    "print(\"Successfully downloaded PlayTennis decision tree as 'playtennis_decision_tree.pdf'.\")\n",
    "\n",
    "# Draw the spam email decision tree and save as PDF\n",
    "spam_email_dot = draw_decision_tree_dictionary(spam_email_tree_dict)\n",
    "spam_email_dot.render(\"emails_spam_tree\", format='pdf', cleanup=True)\n",
    "print(\"Successfully downloaded Spam Email decision tree as 'spam_email_decision_tree.pdf'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f24ed1-0237-4ff6-be26-8cd63b7b0484",
   "metadata": {},
   "source": [
    "5. **Training and Testing**:\n",
    "   - Train the model using `census_training.csv` and evaluate its accuracy on `census_training_test.csv`.\n",
    "   - Print the accuracy of your model by comparing predicted values against the actual class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5edd23-56ac-417d-9f86-f5f4615677f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Census Training Tree (as dictionary):\n",
      "{'feature': 'relationship', 'threshold': 0.5, 'left': {'feature': 'education', 'threshold': 0.5, 'left': {'feature': 'hours_per_week', 'threshold': 2.5, 'left': {'value': [[0.26480716253443526, 0.7351928374655647]]}, 'right': {'value': [[0.5859030837004405, 0.41409691629955947]]}}, 'right': {'feature': 'education_num', 'threshold': 1.5, 'left': {'value': [[0.5887592522895496, 0.4112407477104504]]}, 'right': {'value': [[0.8731454005934718, 0.12685459940652818]]}}}, 'right': {'feature': 'relationship', 'threshold': 4.5, 'left': {'feature': 'hours_per_week', 'threshold': 1.5, 'left': {'value': [[0.8085351787773933, 0.1914648212226067]]}, 'right': {'value': [[0.9533504210911754, 0.0466495789088246]]}}, 'right': {'feature': 'education', 'threshold': 0.5, 'left': {'value': [[0.27055702917771884, 0.7294429708222812]]}, 'right': {'value': [[0.5920155793573515, 0.4079844206426485]]}}}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the census training dataset\n",
    "census_data = pd.read_csv('data/census_training.csv')\n",
    "\n",
    "# Select features and target for training\n",
    "census_features = census_data.drop(columns=['high_income'])  # Drop target column\n",
    "census_target = census_data['high_income']  # Target variable\n",
    "\n",
    "# Encode categorical features using LabelEncoder\n",
    "for column in census_features.select_dtypes(include=['object']).columns:\n",
    "    encoder = LabelEncoder()\n",
    "    census_features[column] = encoder.fit_transform(census_features[column])  # Transform categorical data\n",
    "\n",
    "# Initialize and fit the Decision Tree Classifier\n",
    "census_tree = DecisionTreeClassifier(max_depth=3, random_state=42)  # Set max depth and random state\n",
    "census_tree.fit(census_features, census_target)  # Fit the model\n",
    "\n",
    "# Function to convert the decision tree into a nested dictionary format\n",
    "def tree_to_dict(tree, feature_names):\n",
    "    \"\"\"Convert a DecisionTreeClassifier into a nested dictionary.\"\"\"\n",
    "    from sklearn.tree import _tree  # Import _tree for internal structure\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    def recurse(node):\n",
    "        # Check if the node is a leaf or a split node\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            left = recurse(tree_.children_left[node])  # Recur for left child\n",
    "            right = recurse(tree_.children_right[node])  # Recur for right child\n",
    "            return {\"feature\": name, \"threshold\": threshold, \"left\": left, \"right\": right}\n",
    "        else:\n",
    "            return {\"value\": tree_.value[node].tolist()}  # Return leaf value\n",
    "\n",
    "    return recurse(0)  # Start recursion from the root\n",
    "\n",
    "# Convert the census decision tree to dictionary format\n",
    "census_tree_dict = tree_to_dict(census_tree, census_features.columns)\n",
    "\n",
    "# Print the Census Decision Tree as a dictionary\n",
    "print(\"Census Training Tree (as dictionary):\")\n",
    "print(census_tree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "747794b4-049a-447c-aacd-ab788977cca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of accurate classifications: 12308\n",
      "Number of inaccurate classifications: 2720\n",
      "Accuracy: 81.90%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88     11330\n",
      "           1       0.66      0.55      0.60      3698\n",
      "\n",
      "    accuracy                           0.82     15028\n",
      "   macro avg       0.76      0.73      0.74     15028\n",
      "weighted avg       0.81      0.82      0.81     15028\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'census_training_tree.pdf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv('data/census_training.csv')  # Replace with your training dataset path\n",
    "\n",
    "# Preprocessing: One-hot encoding for categorical variables\n",
    "categorical_features = ['marital_status', 'education_num', 'occupation', \n",
    "                        'relationship', 'native_country', 'sex', 'workclass']\n",
    "X_train = pd.get_dummies(train_data[categorical_features], drop_first=True)\n",
    "y_train = (train_data['high_income'] == '>50K').astype(int)  # Convert target variable to binary\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv('data/census_training_test.csv')  # Replace with your test dataset path\n",
    "X_test = pd.get_dummies(test_data[categorical_features], drop_first=True)\n",
    "y_test = (test_data['high_income'] == '>50K').astype(int)  # Convert target variable to binary\n",
    "\n",
    "# Ensure that X_test has the same columns as X_train\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Train the decision tree without pruning\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Classification report and accuracy\n",
    "accurate_classifications = np.sum(y_pred == y_test)\n",
    "inaccurate_classifications = np.sum(y_pred != y_test)\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "\n",
    "print(f\"Number of accurate classifications: {accurate_classifications}\")\n",
    "print(f\"Number of inaccurate classifications: {inaccurate_classifications}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualizing the decision tree without pruning\n",
    "# Sanitize feature names\n",
    "def sanitize_labels(feature_names):\n",
    "    \"\"\"Replace problematic characters in feature names.\"\"\"\n",
    "    return [name.replace('&', 'and').replace('<', 'less_than').replace('>', 'greater_than') for name in feature_names]\n",
    "\n",
    "# Sanitize feature names\n",
    "sanitized_feature_names = sanitize_labels(X_train.columns)\n",
    "\n",
    "# Export the tree\n",
    "dot_data = export_graphviz(dt_classifier, out_file=None, \n",
    "                           feature_names=sanitized_feature_names,  \n",
    "                           class_names=['less_or_equal_50K', 'more_than_50K'],  \n",
    "                           filled=True, rounded=True,  \n",
    "                           special_characters=False)  # Set to False after sanitizing\n",
    "\n",
    "# Create graph and save as PDF\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"census_training_tree\")  # Save the tree visualization as a PDF\n",
    "graph.view()  # Opens the tree visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126cf024-dfa4-4ab4-8182-33d4e58716a8",
   "metadata": {},
   "source": [
    "6. **Tree Pruning**:\n",
    "   - Implement tree pruning to improve accuracy. Experiment with different thresholds for stopping tree growth and observe the improvements in classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11bad560-4a1f-4052-b070-efc69af510ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of accurate classifications: 12376\n",
      "Number of inaccurate classifications: 2652\n",
      "Accuracy: 82.35%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.89     11330\n",
      "           1       0.67      0.55      0.60      3698\n",
      "\n",
      "    accuracy                           0.82     15028\n",
      "   macro avg       0.77      0.73      0.75     15028\n",
      "weighted avg       0.81      0.82      0.82     15028\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'census_training_tree_pruned.pdf'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv('data/census_training.csv')  # Replace with your training dataset path\n",
    "\n",
    "# Preprocessing: One-hot encoding for categorical variables\n",
    "categorical_features = ['marital_status', 'education_num', 'occupation', \n",
    "                        'relationship', 'native_country', 'sex', 'workclass']\n",
    "X_train = pd.get_dummies(train_data[categorical_features], drop_first=True)\n",
    "y_train = (train_data['high_income'] == '>50K').astype(int)  # Convert target variable to binary\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv('data/census_training_test.csv')  # Replace with your test dataset path\n",
    "X_test = pd.get_dummies(test_data[categorical_features], drop_first=True)\n",
    "y_test = (test_data['high_income'] == '>50K').astype(int)  # Convert target variable to binary\n",
    "\n",
    "# Ensure that X_test has the same columns as X_train\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Train the decision tree with pruning\n",
    "dt_classifier = DecisionTreeClassifier(min_samples_leaf=30, random_state=42)  # Setting the threshold for pruning\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Classification report and accuracy\n",
    "accurate_classifications = np.sum(y_pred == y_test)\n",
    "inaccurate_classifications = np.sum(y_pred != y_test)\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "\n",
    "print(f\"Number of accurate classifications: {accurate_classifications}\")\n",
    "print(f\"Number of inaccurate classifications: {inaccurate_classifications}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualizing the decision tree with pruning\n",
    "# Sanitize feature names\n",
    "def sanitize_labels(feature_names):\n",
    "    \"\"\"Replace problematic characters in feature names.\"\"\"\n",
    "    return [name.replace('&', 'and').replace('<', 'less_than').replace('>', 'greater_than') for name in feature_names]\n",
    "\n",
    "# Sanitize feature names\n",
    "sanitized_feature_names = sanitize_labels(X_train.columns)\n",
    "\n",
    "# Export the tree\n",
    "dot_data = export_graphviz(dt_classifier, out_file=None, \n",
    "                           feature_names=sanitized_feature_names,  \n",
    "                           class_names=['less_or_equal_50K', 'more_than_50K'],  \n",
    "                           filled=True, rounded=True,  \n",
    "                           special_characters=False)  # Set to False after sanitizing\n",
    "\n",
    "# Create graph and save as PDF\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"census_training_tree_pruned\")  # Save the tree visualization as a PDF\n",
    "graph.view()  # Opens the tree visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
